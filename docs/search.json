[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts. Azin is here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Airline Passenger Satisfaction Prediction\n\n\n\n\n\n\nMachin Learning\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nAzin Piran\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 14, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "In this blog, we’ll explore a comprehensive analysis and modeling process aimed at predicting the satisfaction of airline passengers. We’ll delve into data preprocessing, feature engineering, and model evaluation, ultimately identifying the best machine learning algorithm to predict passenger satisfaction. Here’s how we’ll break it down:\n\n\nThe goal of this task is to predict whether a passenger is satisfied or dissatisfied with their flight experience. The dataset we’re using contains information like flight distances, seat comfort, online boarding, and various delays. By predicting satisfaction, we aim to provide valuable insights to airlines, helping them improve customer experience and operational efficiency.\n\n\n\nBefore diving into the machine learning preprocessing, we need to ensure that the data is valid, consistent, and ready for analysis. Below are the essential data validation steps we performed during the data preprocessing phase. The data validation process included:\n\nMissing Value Detection and Imputation\nOutlier Detection\nData Type Validation\nCorrect Category Levels\nChecking Duplicates\nTarget Variable Analysis\n\n\n\n\nWe begin by examining the dataset to understand its structure. This dataset contains both numerical and categorical features related to passenger experiences. Some features include:\n\nGender (categorical)\nFlight distance (numerical)\nSeat comfort (ordinal)\nDeparture delay (numerical)\nSatisfaction (binary target variable: 0 = Dissatisfied, 1 = Satisfied)\n\nWe need to explore the relationships between these features and how they relate to the satisfaction of passengers.\n\n\nWe Plot histograms or boxplots for all numerical features to check their distribution and identify any outliers.\n\n\n\n\nNumerical Feature Distirbution\n\n\n\nThe numeric variables except age are mostly right skewed. So, most of them are not close to normal distribution. Additionally, for some ordinal categorical variables like seat_comfort, on_board_service and inflight_entertainment there are very little observations having value of 0. We may need to handle those observations later.\nWE also used count plots to visualize the distribution of ordinal features, such as seat comfort, inflight wifi service, and others, by satisfaction. This allows us to identify any potential patterns or differences in satisfaction levels across these features.\n\n\n\n\nCategorical Feature Target Plots\n\n\n\n\n\n\nCheck how numerical features correlate with each other, and see if there are any strong correlations. This can be done using a correlation matrix and visualized with a heatmap.\n\n\n\n\nCorrelation Matrix\n\n\n\nSome features have high correlation suggesting multicollinearity. Departure Delay in Minutes vs. Arrival Delay in Minutes are very high correlated features (anomalous correlation, which suggests they both contain the same information, so one of them can be deleted).\n\n\n\n\nData preprocessing is a crucial step in any machine learning project, and we follow these steps to ensure our dataset is ready for modeling:\n\nRemoving Irrelevant Features: We remove the arrival_delay_in_minutes column due to its high correlation with departure_delay_in_minutes.\nEncoding Categorical Variables: We use one-hot encoding to convert categorical features like gender, customer_type, and type_of_travel into numerical representations. The satisfaction column is encoded as a binary variable (0 for dissatisfied, 1 for satisfied).\nScaling Numerical Features: Features such as age, flight_distance, and departure_delay_in_minutes are standardized using StandardScaler. We scale ordinal features like seat_comfort using MinMaxScaler to bring them to a similar scale, ensuring no feature dominates the others during model training.\n\nThe preprocessing pipeline is implemented using ColumnTransformer from scikit-learn. Here’s how the pipeline is structured:\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n    (MinMaxScaler(), ordinal_cols),\n    (StandardScaler(), numerical_cols),\n    ('drop', drop_cols)\n)\n\n\n\nNow that the data is preprocessed, we can move on to modeling. Our problem is a binary classification problem, and we’ll evaluate different models to see which performs best.\n\n\nWe begin by setting a baseline model using DummyClassifier. This classifier predicts the majority class (the most frequent class in the dataset), which helps us establish a baseline accuracy.The baseline model has an accuracy of 56.4%, which corresponds to the percentage of the majority class (dissatisfied passengers) in the dataset.\n\n\n\nWe compare the results of our models, including the baseline DummyClassifier, Logistic Regression, and Decision Tree. The results include metrics such as accuracy, precision, recall, and F1-score for both training and validation data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nfit_time\nscore_time\nvalidation_accuracy\ntrain_accuracy\nvalidation_precision\ntrain_precision\nvalidation_recall\ntrain_recall\nvalidation_f1\ntrain_f1\n\n\n\n\ndummy\n0.134 (+/- 0.011)\n0.472 (+/- 0.017)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.318 (+/- 0.000)\n0.318 (+/- 0.000)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.407 (+/- 0.000)\n0.407 (+/- 0.000)\n\n\nLogistic Regression\n0.338 (+/- 0.015)\n0.475 (+/- 0.025)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n\n\nDecision Tree\n0.519 (+/- 0.024)\n0.489 (+/- 0.020)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.946 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n\n\n\nAs shown in the table, Decision Tree outperforms the other models in terms of accuracy and other evaluation metrics. Although it overfits to the training data, the decision tree’s interpretability makes it an excellent candidate for further tuning.\n\n\n\n\nSince decision trees tend to overfit, we use Grid Search to tune the hyperparameters and reduce overfitting, specifically by adjusting the max_depth parameter.And by exploring different values for max_depth, we can find an optimal configuration that balances model complexity and performance.\n\n\n\nIn this evaluation, the decision tree model demonstrated strong performance, achieving an impressive test accuracy of 95.25%. The Classification Report showed high precision, recall, and F1-scores, especially for the “neutral or dissatisfied” class (recall of 97%). This indicates the model’s robust ability to identify dissatisfied passengers with minimal false negatives. The model’s ability to maintain balanced performance across both classes, with macro and weighted average scores of 95%, highlights its generalization capabilities.\n\n\n\n\n\n\n\n\n\n\nNeutral or Dissatisfied\nSatisfied\nAccuracy\nMacro Avg\nWeighted Avg\n\n\n\n\n0.944092757\n0.95861063\n0.950223283\n0.951351693\n0.950438458\n\n\n0.968950896\n0.926105337\n0.950223283\n0.947528117\n0.950223283\n\n\n0.956360323\n0.942077678\n0.950223283\n0.949219\n0.950117439\n\n\n14622\n11354\n0.950223283\n25976\n25976\n\n\n\nThe Confusion Matrix revealed that while the model performs well overall, there are areas to enhance, such as reducing false positives for the “satisfied” class. Adjusting the decision threshold or rebalancing the data could improve recall for this class.\n\n\n\n\nConfusion Matrix\n\n\n\nThe Precision-Recall and ROC curves also reinforced the model’s consistency and effectiveness in distinguishing between “satisfied” and “neutral or dissatisfied” passengers, with a high AUC score of 0.98.\n\n\n\n\nROC Curve\n\n\n\n\n\n\nIn summary, the decision tree model offers a reliable tool for predicting passenger satisfaction with US airlines. With further fine-tuning, additional feature exploration, and interpretability methods, it can be improved to provide even more accurate and actionable insights for the airline industry.\n\n\n\n\nD, John. 2018. “Passenger Satisfaction.” Kaggle Dataset.\nEshaghi, M. Sadegh, Mona Afshardoost, Gui Lohmann, and Brent D. Moyle. 2024. “Drivers and Outcomes of Airline Passenger Satisfaction: A Meta-Analysis.” Journal of the Air Transport Research Society 3: 100034. DOI.\nHunter, J. D. 2007. “Matplotlib: A 2D Graphics Environment.” Computing in Science & Engineering 9 (3): 90–95. DOI.\nKlein, TJ. 2020. “Airline Passenger Satisfaction.” Kaggle Dataset.\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in Python.” In Proceedings of the 9th Python in Science Conference, edited by Stéfan van der Walt and Jarrod Millman, 51–56.\nNamukasa, J. 2013. “The Influence of Airline Service Quality on Passenger Satisfaction and Loyalty: The Case of Uganda Airline Industry.” The TQM Journal 25 (5): 520–32. DOI.\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\nVan Rossum, Guido, and Fred L Drake Jr. 1995. Python Reference Manual. Centrum voor Wiskunde en Informatica Amsterdam.\n\n\n\n\n\n\nEnsuring the reproducibility of our project is essential for transparency and for others to be able to replicate and build upon our work. Below, we outline the key elements that make this project reproducible.\n\n\n\n\n\nTo run the analysis in a dedicated computational environment set up using Docker, please follow these steps:\nStep 1: Clone the repository Outlined are 2 options for cloning the repository- through https or ssh.\n\nNote: The instructions contained in this section assume the commands are executed in a unix-based shell.\n\nUsing Https:\ngit clone https://github.com/UBC-MDS/airline-customer-satisfaction-predictor.git\nUsing SSH:\ngit clone git@github.com:UBC-MDS/airline-customer-satisfaction-predictor.git\nStep 2: Setup Docker Computational Environment\n\nNavigate to the root directory of the project: In the terminal/command line navigate to the root directory of your local copy of this project. bash     cd &lt;repo_directory&gt;\nLaunch the docker container image for the computational environment:\ndocker-compose up\n\nThe terminal logs should display an output similar to: Jupyter Server 2.14.2 is running at:\nLocate the URL starting with http://127.0.0.1:8888/lab?token= and click (or copy and paste in the browser) on the http address in the logs to access the Jupyter application from your web browser. Example link: http://127.0.0.1:8888/lab?token=9f22c04a7fe732fdb2d2d98f1c2c0b74a89a5a6a1d60b45b\n\n\nStep 3: Run the Analysis\nThe first method (Recommended):\nIn the root directory of the project run the following:\nmake all\nThe Makefile will run all the necessary files to generate the results and the report. This is the recommended option because it checks if all the dependencies have generated for each consecutive step.\nAdditionally, if you want to erase everything generated, you can run the following:\nmake clean"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#understanding-the-problem",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#understanding-the-problem",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "The goal of this task is to predict whether a passenger is satisfied or dissatisfied with their flight experience. The dataset we’re using contains information like flight distances, seat comfort, online boarding, and various delays. By predicting satisfaction, we aim to provide valuable insights to airlines, helping them improve customer experience and operational efficiency."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#data-validation",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#data-validation",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Before diving into the machine learning preprocessing, we need to ensure that the data is valid, consistent, and ready for analysis. Below are the essential data validation steps we performed during the data preprocessing phase. The data validation process included:\n\nMissing Value Detection and Imputation\nOutlier Detection\nData Type Validation\nCorrect Category Levels\nChecking Duplicates\nTarget Variable Analysis"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#exploring-the-dataset-eda",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#exploring-the-dataset-eda",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "We begin by examining the dataset to understand its structure. This dataset contains both numerical and categorical features related to passenger experiences. Some features include:\n\nGender (categorical)\nFlight distance (numerical)\nSeat comfort (ordinal)\nDeparture delay (numerical)\nSatisfaction (binary target variable: 0 = Dissatisfied, 1 = Satisfied)\n\nWe need to explore the relationships between these features and how they relate to the satisfaction of passengers.\n\n\nWe Plot histograms or boxplots for all numerical features to check their distribution and identify any outliers.\n\n\n\n\nNumerical Feature Distirbution\n\n\n\nThe numeric variables except age are mostly right skewed. So, most of them are not close to normal distribution. Additionally, for some ordinal categorical variables like seat_comfort, on_board_service and inflight_entertainment there are very little observations having value of 0. We may need to handle those observations later.\nWE also used count plots to visualize the distribution of ordinal features, such as seat comfort, inflight wifi service, and others, by satisfaction. This allows us to identify any potential patterns or differences in satisfaction levels across these features.\n\n\n\n\nCategorical Feature Target Plots\n\n\n\n\n\n\nCheck how numerical features correlate with each other, and see if there are any strong correlations. This can be done using a correlation matrix and visualized with a heatmap.\n\n\n\n\nCorrelation Matrix\n\n\n\nSome features have high correlation suggesting multicollinearity. Departure Delay in Minutes vs. Arrival Delay in Minutes are very high correlated features (anomalous correlation, which suggests they both contain the same information, so one of them can be deleted)."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#data-preprocessing",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#data-preprocessing",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Data preprocessing is a crucial step in any machine learning project, and we follow these steps to ensure our dataset is ready for modeling:\n\nRemoving Irrelevant Features: We remove the arrival_delay_in_minutes column due to its high correlation with departure_delay_in_minutes.\nEncoding Categorical Variables: We use one-hot encoding to convert categorical features like gender, customer_type, and type_of_travel into numerical representations. The satisfaction column is encoded as a binary variable (0 for dissatisfied, 1 for satisfied).\nScaling Numerical Features: Features such as age, flight_distance, and departure_delay_in_minutes are standardized using StandardScaler. We scale ordinal features like seat_comfort using MinMaxScaler to bring them to a similar scale, ensuring no feature dominates the others during model training.\n\nThe preprocessing pipeline is implemented using ColumnTransformer from scikit-learn. Here’s how the pipeline is structured:\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n    (MinMaxScaler(), ordinal_cols),\n    (StandardScaler(), numerical_cols),\n    ('drop', drop_cols)\n)"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#modeling",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#modeling",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Now that the data is preprocessed, we can move on to modeling. Our problem is a binary classification problem, and we’ll evaluate different models to see which performs best.\n\n\nWe begin by setting a baseline model using DummyClassifier. This classifier predicts the majority class (the most frequent class in the dataset), which helps us establish a baseline accuracy.The baseline model has an accuracy of 56.4%, which corresponds to the percentage of the majority class (dissatisfied passengers) in the dataset.\n\n\n\nWe compare the results of our models, including the baseline DummyClassifier, Logistic Regression, and Decision Tree. The results include metrics such as accuracy, precision, recall, and F1-score for both training and validation data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nfit_time\nscore_time\nvalidation_accuracy\ntrain_accuracy\nvalidation_precision\ntrain_precision\nvalidation_recall\ntrain_recall\nvalidation_f1\ntrain_f1\n\n\n\n\ndummy\n0.134 (+/- 0.011)\n0.472 (+/- 0.017)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.318 (+/- 0.000)\n0.318 (+/- 0.000)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.407 (+/- 0.000)\n0.407 (+/- 0.000)\n\n\nLogistic Regression\n0.338 (+/- 0.015)\n0.475 (+/- 0.025)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n\n\nDecision Tree\n0.519 (+/- 0.024)\n0.489 (+/- 0.020)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.946 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n\n\n\nAs shown in the table, Decision Tree outperforms the other models in terms of accuracy and other evaluation metrics. Although it overfits to the training data, the decision tree’s interpretability makes it an excellent candidate for further tuning."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#hyperparameter-tuning-with-grid-search",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#hyperparameter-tuning-with-grid-search",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Since decision trees tend to overfit, we use Grid Search to tune the hyperparameters and reduce overfitting, specifically by adjusting the max_depth parameter.And by exploring different values for max_depth, we can find an optimal configuration that balances model complexity and performance."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#conclusion",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#conclusion",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "In this evaluation, the decision tree model demonstrated strong performance, achieving an impressive test accuracy of 95.25%. The Classification Report showed high precision, recall, and F1-scores, especially for the “neutral or dissatisfied” class (recall of 97%). This indicates the model’s robust ability to identify dissatisfied passengers with minimal false negatives. The model’s ability to maintain balanced performance across both classes, with macro and weighted average scores of 95%, highlights its generalization capabilities.\n\n\n\n\n\n\n\n\n\n\nNeutral or Dissatisfied\nSatisfied\nAccuracy\nMacro Avg\nWeighted Avg\n\n\n\n\n0.944092757\n0.95861063\n0.950223283\n0.951351693\n0.950438458\n\n\n0.968950896\n0.926105337\n0.950223283\n0.947528117\n0.950223283\n\n\n0.956360323\n0.942077678\n0.950223283\n0.949219\n0.950117439\n\n\n14622\n11354\n0.950223283\n25976\n25976\n\n\n\nThe Confusion Matrix revealed that while the model performs well overall, there are areas to enhance, such as reducing false positives for the “satisfied” class. Adjusting the decision threshold or rebalancing the data could improve recall for this class.\n\n\n\n\nConfusion Matrix\n\n\n\nThe Precision-Recall and ROC curves also reinforced the model’s consistency and effectiveness in distinguishing between “satisfied” and “neutral or dissatisfied” passengers, with a high AUC score of 0.98.\n\n\n\n\nROC Curve"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#summary-and-final-thoughts",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#summary-and-final-thoughts",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "In summary, the decision tree model offers a reliable tool for predicting passenger satisfaction with US airlines. With further fine-tuning, additional feature exploration, and interpretability methods, it can be improved to provide even more accurate and actionable insights for the airline industry."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#references",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#references",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "D, John. 2018. “Passenger Satisfaction.” Kaggle Dataset.\nEshaghi, M. Sadegh, Mona Afshardoost, Gui Lohmann, and Brent D. Moyle. 2024. “Drivers and Outcomes of Airline Passenger Satisfaction: A Meta-Analysis.” Journal of the Air Transport Research Society 3: 100034. DOI.\nHunter, J. D. 2007. “Matplotlib: A 2D Graphics Environment.” Computing in Science & Engineering 9 (3): 90–95. DOI.\nKlein, TJ. 2020. “Airline Passenger Satisfaction.” Kaggle Dataset.\nMcKinney, Wes. 2010. “Data Structures for Statistical Computing in Python.” In Proceedings of the 9th Python in Science Conference, edited by Stéfan van der Walt and Jarrod Millman, 51–56.\nNamukasa, J. 2013. “The Influence of Airline Service Quality on Passenger Satisfaction and Loyalty: The Case of Uganda Airline Industry.” The TQM Journal 25 (5): 520–32. DOI.\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\nVan Rossum, Guido, and Fred L Drake Jr. 1995. Python Reference Manual. Centrum voor Wiskunde en Informatica Amsterdam."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#appendix",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#appendix",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Ensuring the reproducibility of our project is essential for transparency and for others to be able to replicate and build upon our work. Below, we outline the key elements that make this project reproducible.\n\n\n\n\n\nTo run the analysis in a dedicated computational environment set up using Docker, please follow these steps:\nStep 1: Clone the repository Outlined are 2 options for cloning the repository- through https or ssh.\n\nNote: The instructions contained in this section assume the commands are executed in a unix-based shell.\n\nUsing Https:\ngit clone https://github.com/UBC-MDS/airline-customer-satisfaction-predictor.git\nUsing SSH:\ngit clone git@github.com:UBC-MDS/airline-customer-satisfaction-predictor.git\nStep 2: Setup Docker Computational Environment\n\nNavigate to the root directory of the project: In the terminal/command line navigate to the root directory of your local copy of this project. bash     cd &lt;repo_directory&gt;\nLaunch the docker container image for the computational environment:\ndocker-compose up\n\nThe terminal logs should display an output similar to: Jupyter Server 2.14.2 is running at:\nLocate the URL starting with http://127.0.0.1:8888/lab?token= and click (or copy and paste in the browser) on the http address in the logs to access the Jupyter application from your web browser. Example link: http://127.0.0.1:8888/lab?token=9f22c04a7fe732fdb2d2d98f1c2c0b74a89a5a6a1d60b45b\n\n\nStep 3: Run the Analysis\nThe first method (Recommended):\nIn the root directory of the project run the following:\nmake all\nThe Makefile will run all the necessary files to generate the results and the report. This is the recommended option because it checks if all the dependencies have generated for each consecutive step.\nAdditionally, if you want to erase everything generated, you can run the following:\nmake clean"
  }
]