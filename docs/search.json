[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hi, and welcome to my little corner of the internet!\nI created this blog to document my journey in data science, analytics, and continuous learning. Over the years, I‚Äôve worked in data engineering, business intelligence, and recently completed my Master of Data Science (MDS) at UBC. This blog is where I bring all of that together, sharing my experiences, learning in public, and connecting with others who are as passionate about data as I am."
  },
  {
    "objectID": "posts/welcome/index.html#why-i-started-this-blog",
    "href": "posts/welcome/index.html#why-i-started-this-blog",
    "title": "Welcome To My Blog",
    "section": "Why I Started This Blog",
    "text": "Why I Started This Blog\nI strongly believe in the idea of learning by sharing. Writing about what I‚Äôm exploring not only helps me solidify my own understanding, but it also creates a space where others can learn alongside me."
  },
  {
    "objectID": "posts/welcome/index.html#lets-learn-together",
    "href": "posts/welcome/index.html#lets-learn-together",
    "title": "Welcome To My Blog",
    "section": "Let‚Äôs Learn Together!",
    "text": "Let‚Äôs Learn Together!\nWhether you‚Äôre a student, professional, or just curious about data, I hope you find something here that inspires you, teaches you something new, or encourages you to start your own experiments.\nThanks for stopping by and welcome again! Feel free to connect with me through the links below. Let‚Äôs explore, learn, and grow together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Think Data, Build Insights",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 25, 2025\n\n\nAzin Piran\n\n\n\n\n\n\n\n\n\n\n\n\nAirline Passenger Satisfaction Prediction\n\n\n\nMachin Learning\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nAzin Piran\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I‚Äôm Azin üëã. I‚Äôm a Senior Data Manager with a strong background in data engineering, analytics, and BI solutions. Recently, I completed my Master of Data Science (MDS) at UBC, where I specialized in data wrangling, statistical modeling, and cloud-based solutions. I‚Äôm passionate about turning complex data into actionable insights and building scalable data systems, especially in maritime operations."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About Me",
    "section": "About This Blog",
    "text": "About This Blog\nThis blog is a space where I share data science tutorials and technical insights, document my learning journey and projects, and provide practical tips drawn from real-world data challenges. It‚Äôs also where I explore new ideas and experiments in data, while showcasing my skills and knowledge as I continue to learn and grow alongside other data enthusiasts. Whether you‚Äôre a data professional, a student, or simply curious about the field, I hope you‚Äôll find something helpful and inspiring here!"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let‚Äôs Connect",
    "text": "Let‚Äôs Connect\nI‚Äôd love to connect, collaborate, or discuss data-driven solutions.\nCheck out my profiles below."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "In this blog, we‚Äôll explore a comprehensive analysis and modeling process aimed at predicting the satisfaction of airline passengers. We‚Äôll delve into data preprocessing, feature engineering, and model evaluation, ultimately identifying the best machine learning algorithm to predict passenger satisfaction. Here‚Äôs how we‚Äôll break it down:\n\n\nThe goal of this task is to predict whether a passenger is satisfied or dissatisfied with their flight experience. The dataset we‚Äôre using contains information like flight distances, seat comfort, online boarding, and various delays. By predicting satisfaction, we aim to provide valuable insights to airlines, helping them improve customer experience and operational efficiency.\n\n\n\nBefore diving into the machine learning preprocessing, we need to ensure that the data is valid, consistent, and ready for analysis. Below are the essential data validation steps we performed during the data preprocessing phase. The data validation process included:\n\nMissing Value Detection and Imputation\nOutlier Detection\nData Type Validation\nCorrect Category Levels\nChecking Duplicates\nTarget Variable Analysis\n\n\n\n\nWe begin by examining the dataset to understand its structure. This dataset contains both numerical and categorical features related to passenger experiences. Some features include:\n\nGender (categorical)\nFlight distance (numerical)\nSeat comfort (ordinal)\nDeparture delay (numerical)\nSatisfaction (binary target variable: 0 = Dissatisfied, 1 = Satisfied)\n\nWe need to explore the relationships between these features and how they relate to the satisfaction of passengers.\n\n\nWe Plot histograms or boxplots for all numerical features to check their distribution and identify any outliers.\n\n\n\n\nNumerical Feature Distirbution\n\n\n\nThe numeric variables except age are mostly right skewed. So, most of them are not close to normal distribution. Additionally, for some ordinal categorical variables like seat_comfort, on_board_service and inflight_entertainment there are very little observations having value of 0. We may need to handle those observations later.\nWE also used count plots to visualize the distribution of ordinal features, such as seat comfort, inflight wifi service, and others, by satisfaction. This allows us to identify any potential patterns or differences in satisfaction levels across these features.\n\n\n\n\nCategorical Feature Target Plots\n\n\n\n\n\n\nCheck how numerical features correlate with each other, and see if there are any strong correlations. This can be done using a correlation matrix and visualized with a heatmap.\n\n\n\n\nCorrelation Matrix\n\n\n\nSome features have high correlation suggesting multicollinearity. Departure Delay in Minutes vs.¬†Arrival Delay in Minutes are very high correlated features (anomalous correlation, which suggests they both contain the same information, so one of them can be deleted).\n\n\n\n\nData preprocessing is a crucial step in any machine learning project, and we follow these steps to ensure our dataset is ready for modeling:\n\nRemoving Irrelevant Features: We remove the arrival_delay_in_minutes column due to its high correlation with departure_delay_in_minutes.\nEncoding Categorical Variables: We use one-hot encoding to convert categorical features like gender, customer_type, and type_of_travel into numerical representations. The satisfaction column is encoded as a binary variable (0 for dissatisfied, 1 for satisfied).\nScaling Numerical Features: Features such as age, flight_distance, and departure_delay_in_minutes are standardized using StandardScaler. We scale ordinal features like seat_comfort using MinMaxScaler to bring them to a similar scale, ensuring no feature dominates the others during model training.\n\nThe preprocessing pipeline is implemented using ColumnTransformer from scikit-learn. Here‚Äôs how the pipeline is structured:\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n    (MinMaxScaler(), ordinal_cols),\n    (StandardScaler(), numerical_cols),\n    ('drop', drop_cols)\n)\n\n\n\nNow that the data is preprocessed, we can move on to modeling. Our problem is a binary classification problem, and we‚Äôll evaluate different models to see which performs best.\n\n\nWe begin by setting a baseline model using DummyClassifier. This classifier predicts the majority class (the most frequent class in the dataset), which helps us establish a baseline accuracy.The baseline model has an accuracy of 56.4%, which corresponds to the percentage of the majority class (dissatisfied passengers) in the dataset.\n\n\n\nWe compare the results of our models, including the baseline DummyClassifier, Logistic Regression, and Decision Tree. The results include metrics such as accuracy, precision, recall, and F1-score for both training and validation data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nfit_time\nscore_time\nvalidation_accuracy\ntrain_accuracy\nvalidation_precision\ntrain_precision\nvalidation_recall\ntrain_recall\nvalidation_f1\ntrain_f1\n\n\n\n\ndummy\n0.134 (+/- 0.011)\n0.472 (+/- 0.017)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.318 (+/- 0.000)\n0.318 (+/- 0.000)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.407 (+/- 0.000)\n0.407 (+/- 0.000)\n\n\nLogistic Regression\n0.338 (+/- 0.015)\n0.475 (+/- 0.025)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n\n\nDecision Tree\n0.519 (+/- 0.024)\n0.489 (+/- 0.020)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.946 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n\n\n\nAs shown in the table, Decision Tree outperforms the other models in terms of accuracy and other evaluation metrics. Although it overfits to the training data, the decision tree‚Äôs interpretability makes it an excellent candidate for further tuning.\n\n\n\n\nSince decision trees tend to overfit, we use Grid Search to tune the hyperparameters and reduce overfitting, specifically by adjusting the max_depth parameter.And by exploring different values for max_depth, we can find an optimal configuration that balances model complexity and performance.\n\n\n\nIn this evaluation, the decision tree model demonstrated strong performance, achieving an impressive test accuracy of 95.25%. The Classification Report showed high precision, recall, and F1-scores, especially for the ‚Äúneutral or dissatisfied‚Äù class (recall of 97%). This indicates the model‚Äôs robust ability to identify dissatisfied passengers with minimal false negatives. The model‚Äôs ability to maintain balanced performance across both classes, with macro and weighted average scores of 95%, highlights its generalization capabilities.\n\n\n\n\n\n\n\n\n\n\nNeutral or Dissatisfied\nSatisfied\nAccuracy\nMacro Avg\nWeighted Avg\n\n\n\n\n0.944092757\n0.95861063\n0.950223283\n0.951351693\n0.950438458\n\n\n0.968950896\n0.926105337\n0.950223283\n0.947528117\n0.950223283\n\n\n0.956360323\n0.942077678\n0.950223283\n0.949219\n0.950117439\n\n\n14622\n11354\n0.950223283\n25976\n25976\n\n\n\nThe Confusion Matrix revealed that while the model performs well overall, there are areas to enhance, such as reducing false positives for the ‚Äúsatisfied‚Äù class. Adjusting the decision threshold or rebalancing the data could improve recall for this class.\n\n\n\n\nConfusion Matrix\n\n\n\nThe Precision-Recall and ROC curves also reinforced the model‚Äôs consistency and effectiveness in distinguishing between ‚Äúsatisfied‚Äù and ‚Äúneutral or dissatisfied‚Äù passengers, with a high AUC score of 0.98.\n\n\n\n\nROC Curve\n\n\n\n\n\n\nIn summary, the decision tree model offers a reliable tool for predicting passenger satisfaction with US airlines. With further fine-tuning, additional feature exploration, and interpretability methods, it can be improved to provide even more accurate and actionable insights for the airline industry.\n\n\n\n\nD, John. 2018. ‚ÄúPassenger Satisfaction.‚Äù Kaggle Dataset.\nEshaghi, M. Sadegh, Mona Afshardoost, Gui Lohmann, and Brent D. Moyle. 2024. ‚ÄúDrivers and Outcomes of Airline Passenger Satisfaction: A Meta-Analysis.‚Äù Journal of the Air Transport Research Society 3: 100034. DOI.\nHunter, J. D. 2007. ‚ÄúMatplotlib: A 2D Graphics Environment.‚Äù Computing in Science & Engineering 9 (3): 90‚Äì95. DOI.\nKlein, TJ. 2020. ‚ÄúAirline Passenger Satisfaction.‚Äù Kaggle Dataset.\nMcKinney, Wes. 2010. ‚ÄúData Structures for Statistical Computing in Python.‚Äù In Proceedings of the 9th Python in Science Conference, edited by St√©fan van der Walt and Jarrod Millman, 51‚Äì56.\nNamukasa, J. 2013. ‚ÄúThe Influence of Airline Service Quality on Passenger Satisfaction and Loyalty: The Case of Uganda Airline Industry.‚Äù The TQM Journal 25 (5): 520‚Äì32. DOI.\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al.¬†2011. ‚ÄúScikit-Learn: Machine Learning in Python.‚Äù Journal of Machine Learning Research 12: 2825‚Äì30.\nVan Rossum, Guido, and Fred L Drake Jr.¬†1995. Python Reference Manual. Centrum voor Wiskunde en Informatica Amsterdam.\n\n\n\n\n\n\nEnsuring the reproducibility of our project is essential for transparency and for others to be able to replicate and build upon our work. Below, we outline the key elements that make this project reproducible.\n\n\n\n\n\nTo run the analysis in a dedicated computational environment set up using Docker, please follow these steps:\nStep 1: Clone the repository Outlined are 2 options for cloning the repository- through https or ssh.\n\nNote: The instructions contained in this section assume the commands are executed in a unix-based shell.\n\nUsing Https:\ngit clone https://github.com/UBC-MDS/airline-customer-satisfaction-predictor.git\nUsing SSH:\ngit clone git@github.com:UBC-MDS/airline-customer-satisfaction-predictor.git\nStep 2: Setup Docker Computational Environment\n\nNavigate to the root directory of the project: In the terminal/command line navigate to the root directory of your local copy of this project. bash     cd &lt;repo_directory&gt;\nLaunch the docker container image for the computational environment:\ndocker-compose up\n\nThe terminal logs should display an output similar to: Jupyter Server 2.14.2 is running at:\nLocate the URL starting with http://127.0.0.1:8888/lab?token= and click (or copy and paste in the browser) on the http address in the logs to access the Jupyter application from your web browser. Example link: http://127.0.0.1:8888/lab?token=9f22c04a7fe732fdb2d2d98f1c2c0b74a89a5a6a1d60b45b\n\n\nStep 3: Run the Analysis\nThe first method (Recommended):\nIn the root directory of the project run the following:\nmake all\nThe Makefile will run all the necessary files to generate the results and the report. This is the recommended option because it checks if all the dependencies have generated for each consecutive step.\nAdditionally, if you want to erase everything generated, you can run the following:\nmake clean"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#understanding-the-problem",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#understanding-the-problem",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "The goal of this task is to predict whether a passenger is satisfied or dissatisfied with their flight experience. The dataset we‚Äôre using contains information like flight distances, seat comfort, online boarding, and various delays. By predicting satisfaction, we aim to provide valuable insights to airlines, helping them improve customer experience and operational efficiency."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#data-validation",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#data-validation",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Before diving into the machine learning preprocessing, we need to ensure that the data is valid, consistent, and ready for analysis. Below are the essential data validation steps we performed during the data preprocessing phase. The data validation process included:\n\nMissing Value Detection and Imputation\nOutlier Detection\nData Type Validation\nCorrect Category Levels\nChecking Duplicates\nTarget Variable Analysis"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#exploring-the-dataset-eda",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#exploring-the-dataset-eda",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "We begin by examining the dataset to understand its structure. This dataset contains both numerical and categorical features related to passenger experiences. Some features include:\n\nGender (categorical)\nFlight distance (numerical)\nSeat comfort (ordinal)\nDeparture delay (numerical)\nSatisfaction (binary target variable: 0 = Dissatisfied, 1 = Satisfied)\n\nWe need to explore the relationships between these features and how they relate to the satisfaction of passengers.\n\n\nWe Plot histograms or boxplots for all numerical features to check their distribution and identify any outliers.\n\n\n\n\nNumerical Feature Distirbution\n\n\n\nThe numeric variables except age are mostly right skewed. So, most of them are not close to normal distribution. Additionally, for some ordinal categorical variables like seat_comfort, on_board_service and inflight_entertainment there are very little observations having value of 0. We may need to handle those observations later.\nWE also used count plots to visualize the distribution of ordinal features, such as seat comfort, inflight wifi service, and others, by satisfaction. This allows us to identify any potential patterns or differences in satisfaction levels across these features.\n\n\n\n\nCategorical Feature Target Plots\n\n\n\n\n\n\nCheck how numerical features correlate with each other, and see if there are any strong correlations. This can be done using a correlation matrix and visualized with a heatmap.\n\n\n\n\nCorrelation Matrix\n\n\n\nSome features have high correlation suggesting multicollinearity. Departure Delay in Minutes vs.¬†Arrival Delay in Minutes are very high correlated features (anomalous correlation, which suggests they both contain the same information, so one of them can be deleted)."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#data-preprocessing",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#data-preprocessing",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Data preprocessing is a crucial step in any machine learning project, and we follow these steps to ensure our dataset is ready for modeling:\n\nRemoving Irrelevant Features: We remove the arrival_delay_in_minutes column due to its high correlation with departure_delay_in_minutes.\nEncoding Categorical Variables: We use one-hot encoding to convert categorical features like gender, customer_type, and type_of_travel into numerical representations. The satisfaction column is encoded as a binary variable (0 for dissatisfied, 1 for satisfied).\nScaling Numerical Features: Features such as age, flight_distance, and departure_delay_in_minutes are standardized using StandardScaler. We scale ordinal features like seat_comfort using MinMaxScaler to bring them to a similar scale, ensuring no feature dominates the others during model training.\n\nThe preprocessing pipeline is implemented using ColumnTransformer from scikit-learn. Here‚Äôs how the pipeline is structured:\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n\npreprocessor = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n    (MinMaxScaler(), ordinal_cols),\n    (StandardScaler(), numerical_cols),\n    ('drop', drop_cols)\n)"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#modeling",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#modeling",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Now that the data is preprocessed, we can move on to modeling. Our problem is a binary classification problem, and we‚Äôll evaluate different models to see which performs best.\n\n\nWe begin by setting a baseline model using DummyClassifier. This classifier predicts the majority class (the most frequent class in the dataset), which helps us establish a baseline accuracy.The baseline model has an accuracy of 56.4%, which corresponds to the percentage of the majority class (dissatisfied passengers) in the dataset.\n\n\n\nWe compare the results of our models, including the baseline DummyClassifier, Logistic Regression, and Decision Tree. The results include metrics such as accuracy, precision, recall, and F1-score for both training and validation data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nfit_time\nscore_time\nvalidation_accuracy\ntrain_accuracy\nvalidation_precision\ntrain_precision\nvalidation_recall\ntrain_recall\nvalidation_f1\ntrain_f1\n\n\n\n\ndummy\n0.134 (+/- 0.011)\n0.472 (+/- 0.017)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.318 (+/- 0.000)\n0.318 (+/- 0.000)\n0.564 (+/- 0.000)\n0.564 (+/- 0.000)\n0.407 (+/- 0.000)\n0.407 (+/- 0.000)\n\n\nLogistic Regression\n0.338 (+/- 0.015)\n0.475 (+/- 0.025)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n0.874 (+/- 0.003)\n0.874 (+/- 0.001)\n\n\nDecision Tree\n0.519 (+/- 0.024)\n0.489 (+/- 0.020)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.946 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n0.945 (+/- 0.001)\n1.000 (+/- 0.000)\n\n\n\nAs shown in the table, Decision Tree outperforms the other models in terms of accuracy and other evaluation metrics. Although it overfits to the training data, the decision tree‚Äôs interpretability makes it an excellent candidate for further tuning."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#hyperparameter-tuning-with-grid-search",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#hyperparameter-tuning-with-grid-search",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Since decision trees tend to overfit, we use Grid Search to tune the hyperparameters and reduce overfitting, specifically by adjusting the max_depth parameter.And by exploring different values for max_depth, we can find an optimal configuration that balances model complexity and performance."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#conclusion",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#conclusion",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "In this evaluation, the decision tree model demonstrated strong performance, achieving an impressive test accuracy of 95.25%. The Classification Report showed high precision, recall, and F1-scores, especially for the ‚Äúneutral or dissatisfied‚Äù class (recall of 97%). This indicates the model‚Äôs robust ability to identify dissatisfied passengers with minimal false negatives. The model‚Äôs ability to maintain balanced performance across both classes, with macro and weighted average scores of 95%, highlights its generalization capabilities.\n\n\n\n\n\n\n\n\n\n\nNeutral or Dissatisfied\nSatisfied\nAccuracy\nMacro Avg\nWeighted Avg\n\n\n\n\n0.944092757\n0.95861063\n0.950223283\n0.951351693\n0.950438458\n\n\n0.968950896\n0.926105337\n0.950223283\n0.947528117\n0.950223283\n\n\n0.956360323\n0.942077678\n0.950223283\n0.949219\n0.950117439\n\n\n14622\n11354\n0.950223283\n25976\n25976\n\n\n\nThe Confusion Matrix revealed that while the model performs well overall, there are areas to enhance, such as reducing false positives for the ‚Äúsatisfied‚Äù class. Adjusting the decision threshold or rebalancing the data could improve recall for this class.\n\n\n\n\nConfusion Matrix\n\n\n\nThe Precision-Recall and ROC curves also reinforced the model‚Äôs consistency and effectiveness in distinguishing between ‚Äúsatisfied‚Äù and ‚Äúneutral or dissatisfied‚Äù passengers, with a high AUC score of 0.98.\n\n\n\n\nROC Curve"
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#summary-and-final-thoughts",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#summary-and-final-thoughts",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "In summary, the decision tree model offers a reliable tool for predicting passenger satisfaction with US airlines. With further fine-tuning, additional feature exploration, and interpretability methods, it can be improved to provide even more accurate and actionable insights for the airline industry."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#references",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#references",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "D, John. 2018. ‚ÄúPassenger Satisfaction.‚Äù Kaggle Dataset.\nEshaghi, M. Sadegh, Mona Afshardoost, Gui Lohmann, and Brent D. Moyle. 2024. ‚ÄúDrivers and Outcomes of Airline Passenger Satisfaction: A Meta-Analysis.‚Äù Journal of the Air Transport Research Society 3: 100034. DOI.\nHunter, J. D. 2007. ‚ÄúMatplotlib: A 2D Graphics Environment.‚Äù Computing in Science & Engineering 9 (3): 90‚Äì95. DOI.\nKlein, TJ. 2020. ‚ÄúAirline Passenger Satisfaction.‚Äù Kaggle Dataset.\nMcKinney, Wes. 2010. ‚ÄúData Structures for Statistical Computing in Python.‚Äù In Proceedings of the 9th Python in Science Conference, edited by St√©fan van der Walt and Jarrod Millman, 51‚Äì56.\nNamukasa, J. 2013. ‚ÄúThe Influence of Airline Service Quality on Passenger Satisfaction and Loyalty: The Case of Uganda Airline Industry.‚Äù The TQM Journal 25 (5): 520‚Äì32. DOI.\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al.¬†2011. ‚ÄúScikit-Learn: Machine Learning in Python.‚Äù Journal of Machine Learning Research 12: 2825‚Äì30.\nVan Rossum, Guido, and Fred L Drake Jr.¬†1995. Python Reference Manual. Centrum voor Wiskunde en Informatica Amsterdam."
  },
  {
    "objectID": "posts/Airline Passenger Satisfaction Prediction/index.html#appendix",
    "href": "posts/Airline Passenger Satisfaction Prediction/index.html#appendix",
    "title": "Airline Passenger Satisfaction Prediction",
    "section": "",
    "text": "Ensuring the reproducibility of our project is essential for transparency and for others to be able to replicate and build upon our work. Below, we outline the key elements that make this project reproducible.\n\n\n\n\n\nTo run the analysis in a dedicated computational environment set up using Docker, please follow these steps:\nStep 1: Clone the repository Outlined are 2 options for cloning the repository- through https or ssh.\n\nNote: The instructions contained in this section assume the commands are executed in a unix-based shell.\n\nUsing Https:\ngit clone https://github.com/UBC-MDS/airline-customer-satisfaction-predictor.git\nUsing SSH:\ngit clone git@github.com:UBC-MDS/airline-customer-satisfaction-predictor.git\nStep 2: Setup Docker Computational Environment\n\nNavigate to the root directory of the project: In the terminal/command line navigate to the root directory of your local copy of this project. bash     cd &lt;repo_directory&gt;\nLaunch the docker container image for the computational environment:\ndocker-compose up\n\nThe terminal logs should display an output similar to: Jupyter Server 2.14.2 is running at:\nLocate the URL starting with http://127.0.0.1:8888/lab?token= and click (or copy and paste in the browser) on the http address in the logs to access the Jupyter application from your web browser. Example link: http://127.0.0.1:8888/lab?token=9f22c04a7fe732fdb2d2d98f1c2c0b74a89a5a6a1d60b45b\n\n\nStep 3: Run the Analysis\nThe first method (Recommended):\nIn the root directory of the project run the following:\nmake all\nThe Makefile will run all the necessary files to generate the results and the report. This is the recommended option because it checks if all the dependencies have generated for each consecutive step.\nAdditionally, if you want to erase everything generated, you can run the following:\nmake clean"
  }
]